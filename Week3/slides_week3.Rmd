---
title: "MXB344 Week 3 Slides"
author: "Miles McBain"
date: "6 August 2016"
output:   
  ioslides_presentation:
    css: ../style.css
---
```{r, include=FALSE}
library(AER)
library(ggplot2)
library(dplyr)
library(plotly)
data(DoctorVisits)
```


#Welcome 

MXB344 Lecture 3

## Recapping | Exponential Family
* Last lecture we dug into the how the exponential family fits into the GLM framework
    + The [exponential family](https://en.wikipedia.org/wiki/Exponential_family) contains all your usual favourites: Exponential, Normal, Poisson, Gamma, Beta, Binomial, etc.
    + This course will deal mainly with Poisson and Binomial. Your assignments will apply these.

## Recall From Last Week... | Scaled Residual Deviance

$$\hat{D}* = \frac{2( l(\tilde{\mu}; y, \phi) - l(\hat{\mu}; y, \phi))}{\phi}$$

$$\hat{D}* = -\frac{2}{\phi} log \left( \frac{L(\hat{\mu}; y, \phi)}{L(\tilde{\mu}; y, \phi)}\right)$$

**Note**: I think I had the ratio upsidedown last week.

## Making it concrete | Normal Example
We show that if $Y_{i} \sim N(\mu_{i}, \sigma^{2})$:

$$\hat{D}* = \sum\left(\frac{y_{i} - \hat{\mu_{i}}}{\sigma}\right)^{2}$$

Does this look familiar?

## Making it concrete | Poisson Example
We show that if $Y_{i} \sim Pois(\lambda_{i})$:

$$\hat{D}* = 2\sum y_{i}log\left(\frac{\hat{\lambda_{i}}}{y_{i}}\right) - (y_{i} - \hat{\lambda_{i}})$$

See unscaled deviance on p115 of MAB624 notes for other exponential family forms. 

## Hypothesis Testing | Using the Scaled Residual Deviance
* $\hat{D*}$ is assymptotically distributed $\chi^{2}$
* We can use this to test a variety of hypotheses.
    + We're approximating! There is no p-value cutoff where the model is suddenly validated. A hypothesis test merely provides evidence of fit quality.

##A basic general linear model {.codefont}
**Note:** R repots the unit deviance.
```{r, echo=FALSE}
poisson_glm_fit <- glm(data = DoctorVisits,
   formula = visits ~ age + income + gender,
   family = poisson(link="log") )

summary(poisson_glm_fit)
```
## What we can ask
* Is there evidence the deviance of our model from the saturated model is significant? (Testing for good fit)
    + Test stat: $\hat{D*}$
* Is there evidence the deviance of our model from the null model is significant? (Testing for lack of fit)
    + Test stat: $\hat{D*}_{null} - \hat{D*}_{m}$
* Is there evidence that the devicance of our model is different to another nested model? (Testing for usefulness of covariate)
    + Test stat: $\hat{D*}_{m0} - \hat{D*}_{m1}$, where $p_{1} > p_{0}$
    
## Testing for goodness of fit
$H_{0}:$ The model does not have significantly more deviance than the saturated model.

$H_{1}:$ The model has significantly more deviance than the saturated model.

$$T = \hat{D*}$$
$$T \sim \chi^{2}(N - p)$$

* If we accept $H_{1}$, then there is evidence our model does not explain variation in the $y_{i}$'s as well as a saturated model. 
    + In practice this will almost always be the case. Increasing p not necessarily the answer.

## Testing for goodness of fit | Example {.codefont}
```{r}
pchisq(q = poisson_glm_fit$deviance/1,  #phi = 1 here.
       df = nrow(DoctorVisits) - length(poisson_glm_fit$coefficients),
       #Num observations - #Num parmeters (including intercept!)
       lower.tail = FALSE)
```

* Do we reject $H_{0}$?

## Testing for lack of fit
$H_{0}:$ The model does not have significantly less deviance than the null model.

$H_{1}:$ The model has significantly less deviance than the null model.

$$T = \hat{D*}_{null} - \hat{D*}_{m}$$
$$T \sim \chi^{2}( (N - 1)-(N-p_{m})) = \chi^{2}(p_{m}-1)$$

* If we can accept $H_{1}$, our model is better than a mean only model. We found some signal associated with our covariates and avoid total failure.

## Testing for lack of fit | Example {.codefont}
```{r}
pchisq(q = (poisson_glm_fit$null.deviance - poisson_glm_fit$deviance),
       df = length(poisson_glm_fit$coefficients) - 1,
       #Difference in number of parameters. The null model has 1.
       lower.tail = FALSE)
```

* Do we reject $H_{0}$?

## Testing a Covariate
Asumming we have two models $m0$ and $m1$, such that $m1$ has one additional covariate included in the model:

$H_{0}:$ Having covariate in the model does not decrease the deviance from the saturated model significanctly, given the effects of other covariates.

$H_{1}:$ Having covariate in the model decreases deviance from saturated model significantly, given the effects of other covariates.

$$T = \hat{D*}_{m0} - \hat{D*}_{m1}$$
$$T \sim \chi^{2}( (N - p_{m0})-(N-p_{m1})) = \chi^{2}(p_{m1}-p_{m0})$$


## Testing a Covariate | Interpretation
* If we accept $H_{1}$, then there is evidence that it is worthwhile to select the covariate in the larger model.
* We can also test groups of covariates in this manner, but we gain less information. Not recommended.
* Testing in this manner does not allow us to make inference about the association between $Y$ and our covariate of interest.
    + If the extra covariate in $m1$ is correlated with a covariate in $m0$ it may not explain much **additional** variance.

## Testing a Covariate | Example
Given our example model: $log(visits) = \beta_{0} + age \beta_{1} + income \beta_{2} + age \beta_{3}$

We add an additional parameter from `DoctorVisits` and test the using change in residual deviance.
```{r, eval=FALSE, include=FALSE}
head(DoctorVisits)
help("DoctorVisits")

poisson_glm_fit_health <- glm(data = DoctorVisits,
   formula = visits ~ age + income + gender + illness,
   family = poisson(link="log") )

summary(poisson_glm_fit_health)

anova_fit <- anova(poisson_glm_fit_health)
anova_fit

pchisq(278.970, 1, lower.tail = FALSE)

poisson_glm_fit_age_health <- glm(data = DoctorVisits,
   formula = visits ~ age + health + income + gender,
   family = poisson(link="log") )

anova_fit2 <- anova(poisson_glm_fit_age_health )
anova_fit2

pchisq(354.29, 1, lower.tail = FALSE)

#Check this!
anova(poisson_glm_fit, poisson_glm_fit_age_health, test="Chisq")



test_grid <- seq(1,10,0.1)
plot(test_grid, pchisq(test_grid,1,lower.tail = FALSE), type="l")
summary(poisson_glm_fit_age_health)
```

## Significance stars

* R assigns signifcance stars using the Wald test. Testing using deviance is preferred to this.
* `z value` comes from:
$$z = \frac{\beta_{i} - 0}{SE_{\beta_{i}}}$$
* `Pr(>|z|)` is the two tailed probability associated with $z$ assuming $z \sim N(0,1)$
    + An CLT-based Normal approximation. Doesn't use T, because that only applies for specific likelihoods.
* Testing $H_{0}$: $\beta_{i}$ = 0

See p121 MAB624 notes - but recall relationship between $N(0,1)$ and $\chi^{2}(1)$!

##Akaike information criterion | Our preferred goodness of fit measure
p124 MAB624 notes

$$AIC = -2l(\hat{\mu}; y, \phi) + 2p$$

* Function of maximised log likelihood for $X^{T}\beta$. 
    + Similar idea to deviance.
* Uses penalty factor for number of parameters to make nested models 'fairly' comparable.
* Smaller is better.

## Stepwise Model selection using AIC | Example
We use `step` to automate model selection from a parameter space based on AIC.
```{r, include=FALSE}
poisson_glm_fit_all <- glm(data = DoctorVisits,
   formula = visits ~ ., #. means everything that isn't the response
   family = poisson(link="log") )

poisson_glm_fit_all_step <- step(object = poisson_glm_fit_all,
                                 scope = visits ~ .
                            )

summary(poisson_glm_fit_all_step)

```

# GLM Residuals

## Types of Residuals
* It is desirable to have a residual which represents each observation's contribution to the overall deviance, so we can judge where poor fit quality occurrs. 
* **Raw Residuals** i.e. $y_{i} - \mu_{i}$ do not have this property because in GLMs the variance around each $\mu_{i}$ is not assumed to be constant e.g. Poisson case.
* There are actually many ways to define resisuals, each of which have some useful properties. We will use:
    + The **Deviance Residual** - measures each observation's contribution to **unit** deviance.
    + The **Pearson Residual** - raw residual scaled by std dev. associated with each $\mu_{i}$. Measures contribution to pearson statistic (Chi-squared) for goodness of fit.
    
##Deviance residuals
$$r_{i}^{D} = sign(y_{i}-\mu_{i})\sqrt{w_{i}d(y_{i},\hat{\mu{i}})}$$
$$D = \sum (r_{i}^{D})^{2}$$

```{r}
deviance_residuals <- residuals(poisson_glm_fit, type = "deviance")
unit_deviance <- sum(deviance_residuals^2)
deviance(poisson_glm_fit) == unit_deviance
```

##Pearson residuals
$$r_{i}^{P} = \frac{(y_{i}-\mu_{i})^{2}}{\frac{1}{w_{i}}V(\hat{\mu}_{i})}$$
$$X^{2} = \sum (r_{i}^{P})^{2}$$
$$\frac{X^{2}}{\phi} \sim \chi^{2}(N-p)$$

***Note:*** Since $E[\chi^{2}(N-p)] = N-p$, $\frac{X^{2}}{N - p} \approx \phi$ is a handy check for dispersion issues.

```{r}
pearson_residuals <- residuals(poisson_glm_fit, type = "pearson")
```


#Poisson Regression | Or My Favourite Regression

##Applications for Poisson Regression
* As we have said many times already the Possion GLMs are associated with count data.
* Count data may take a number a number of forms depending on how the data were caputed.

##Count data | Fixed Exposure
* **Exposure** refers to length of the observation interval in time/space/clicks/views/customers etc..
* Fixed exposure means it is the same for each observation
* Example data: [Number of tropical cyclones in Australian Region by year](http://www.bom.gov.au/cyclone/climatology/trends.shtml)

$$Y_{i} \sim Pois(\lambda_{i})$$ 
$$\mu_{i} = \lambda_{i}$$
$$g(\mu_{i}) = \eta_i$$

for $g(.) = log(.)$ (canonical): $\mu_{i} = \exp(\eta_i)$  

##Count data | Variable Exposure
* Variable exposure means we have some observation interval measure associated with each $y_{i}$
    + $n_{i}$, $t_{i}$... etc
* Example data: `AER::ShipAccidents` (next slide)

##Shipping Accidents

```{r, echo=FALSE}
data("ShipAccidents")
p <- ShipAccidents %>% 
  ggplot(aes(x=service, y=incidents, construction=construction)) + 
  xlab("service (months)") + 
  geom_point(aes(colour=type))
ggplotly(p)
```



##Variable Exposure Cont.
When exposure is variable, we effectively model the **rate**, $\frac{Y_{i}}{n_{i}}$, to weigh each observation the same:

$$\lambda_{i} = E\left[\frac{Y_{i}}{n_{i}}\right]  = \frac{E[Y_{i}]}{n_{i}} = \frac{\mu_{i}}{n_{i}}$$ 
$$g\left(\frac{\mu_{i}}{n_{i}}\right) = \eta_i$$

for $g(.) = log(.)$ (canonical): $$log(\mu_{i}) = log(n_{i}) + \eta_i$$
$$\mu_{i} = \lambda_{i} = n_{i}\exp(\eta_{i})$$

##Fitting with exposure (offset) {.codefont}
* The exposure measure is often called the **offset** since you can imagine it as an adjustment to the intercept for each $y_i$

```{r, include=TRUE, eval=FALSE}
sa <- ShipAccidents %>% filter(service > 0)

sa_full <- glm(data = sa,
                formula = incidents ~ type + construction + operation, 
                family = poisson(link="log"),
                offset = log(service)
              )

summary(sa_full)
```

##Fitting with exposure (offset) {.codefont}

```{r, echo=FALSE, eval=TRUE}
sa <- ShipAccidents %>% filter(service > 0)

sa_full <- glm(data = sa,
                formula = incidents ~ type + construction + operation, 
                family = poisson(link="log"),
                offset = log(service)
              )

summary(sa_full)
```

## Danger: Rate without an exposure
* You may encounter data where the response has "helpfully" been changed to a rate, but the exposure has been lost.
* Modelling this is dangerous. For example an estimated rate of 1/3 from exposure 3 has much higher error than 100/300 from 300 observations, but that information is lost.
* If you suspect exposure varies significantly proceed with caution.